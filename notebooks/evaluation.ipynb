{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: BertNer and FlairNer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages, models, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# models\n",
    "from danlp.models import load_bert_ner_model, load_flair_ner_model\n",
    "\n",
    "# dataset\n",
    "from danlp.datasets import DDT\n",
    "\n",
    "# utils\n",
    "from flair.data import Sentence, Token\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-12 12:28:45,599 loading file /Users/jorgentaule/.danlp/flair.ner.pt\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "bert = load_bert_ner_model()\n",
    "flair = load_flair_ner_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data (splitted into a training set, a validation set, and a test set)\n",
    "ddt = DDT()\n",
    "train, valid, test = ddt.load_as_simple_ner(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the observations and the targets of the testset into new variables\n",
    "sentences, categories = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time models and get their test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_predictions():\n",
    "    start = time.time()\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        predictions.append(bert.predict(sentence)[1])\n",
    "    \n",
    "    time_spent = time.time()-start\n",
    "    \n",
    "    return predictions, time_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preds, bert_time_spent = get_bert_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 70.5121967792511, time per sentence: 0.12480034828186035\n"
     ]
    }
   ],
   "source": [
    "print(f'Time: {bert_time_spent}, time per sentence: {bert_time_spent/len(bert_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flair_predictions():\n",
    "    start = time.time()\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    flair_sentences = []\n",
    "    for sentence in sentences:\n",
    "        flair_sentence = Sentence()\n",
    "        for token in sentence:\n",
    "            flair_sentence.add_token(Token(token))\n",
    "        flair_sentences.append(flair_sentence)\n",
    "    flair.predict(flair_sentences)\n",
    "    \n",
    "    for s in flair_sentences:\n",
    "        predicted_categories = []\n",
    "        for t in s:\n",
    "            predicted_categories.append(t.tags['ner'].value)\n",
    "        predictions.append(predicted_categories)\n",
    "    \n",
    "    time_spent = time.time()-start\n",
    "    \n",
    "    return predictions, time_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgentaule/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/flair/embeddings.py:355: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  word_embedding, device=flair.device, dtype=torch.float\n"
     ]
    }
   ],
   "source": [
    "flair_preds, flair_time_spent = get_flair_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39.77071785926819, time per sentence: 0.07039065107835078\n"
     ]
    }
   ],
   "source": [
    "print(f'Time: {flair_time_spent}, time per sentence: {flair_time_spent/len(flair_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy, precision, recall og f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove MISC from the dataset, as this is not predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(categories):\n",
    "    for j, token in enumerate(sentence):\n",
    "        if token=='I-MISC' or token=='B-MISC':\n",
    "            categories[i][j] = 'O'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which situations are there?\n",
    "\n",
    "- Look at all the predictions similarly.\n",
    "- Only look at PER, ORG, LOC separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some definitions\n",
    "\n",
    "- Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "- Precision = TP/(TP+FP)\n",
    "- Recall = TP/(TP+FN)\n",
    "- F1 Score = 2 * (Recall * Precision) / (Recall + Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_preds_flattened = [item for sublist in flair_preds for item in sublist]\n",
    "bert_preds_flattened = [item for sublist in bert_preds for item in sublist]\n",
    "categories_flattened = [item for sublist in categories for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']\n",
    "bert_rep = classification_report(categories_flattened, bert_preds_flattened, labels=labels)\n",
    "flair_rep = classification_report(categories_flattened, flair_preds_flattened, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.99      1.00      1.00      9383\n",
      "       B-PER       0.93      0.93      0.93       180\n",
      "       I-PER       0.98      1.00      0.99       138\n",
      "       B-LOC       0.80      0.90      0.84        96\n",
      "       I-LOC       0.19      0.80      0.31         5\n",
      "       B-ORG       0.86      0.66      0.75       161\n",
      "       I-ORG       0.94      0.55      0.69        60\n",
      "\n",
      "    accuracy                           0.99     10023\n",
      "   macro avg       0.81      0.83      0.79     10023\n",
      "weighted avg       0.99      0.99      0.99     10023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bert_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.99      1.00      0.99      9383\n",
      "       B-PER       0.92      0.94      0.93       180\n",
      "       I-PER       0.97      1.00      0.98       138\n",
      "       B-LOC       0.86      0.85      0.86        96\n",
      "       I-LOC       1.00      0.40      0.57         5\n",
      "       B-ORG       0.90      0.50      0.65       161\n",
      "       I-ORG       0.91      0.70      0.79        60\n",
      "\n",
      "    accuracy                           0.99     10023\n",
      "   macro avg       0.94      0.77      0.83     10023\n",
      "weighted avg       0.99      0.99      0.98     10023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(flair_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output model mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faulty_sentences(preds):\n",
    "    fault_indices = []\n",
    "    fault_pred_sents = []\n",
    "    \n",
    "    for i, sentence in enumerate(categories):\n",
    "        if sentence!=preds[i]:\n",
    "            fault_indices.append(i)\n",
    "            fault_pred_sents.append(preds[i])\n",
    "            \n",
    "    return fault_indices, fault_pred_sents\n",
    "\n",
    "flair_fault_indices, flair_fault_pred_sents = faulty_sentences(flair_preds)\n",
    "bert_fault_indices, bert_fault_pred_sents = faulty_sentences(bert_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 565)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_fault_indices), len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O\n"
     ]
    }
   ],
   "source": [
    "separator = ','\n",
    "print(separator.join(bert_fault_pred_sents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bert_fault_indices)):\n",
    "    print(categories[bert_fault_indices[i]]==bert_fault_pred_sents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting number of intances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(token, token_prediction, metric_dict):\n",
    "    # count all incorrect predictions\n",
    "    if token!=token_prediction:\n",
    "        metric_dict['total']['incorrect'] += 1\n",
    "        if token!='O':\n",
    "            metric_dict[token[2:]]['incorrect'] += 1\n",
    "    \n",
    "    # both are 'O': add to correct\n",
    "    if token=='O' and token_prediction=='O':\n",
    "        metric_dict['total']['correct'] += 1\n",
    "    \n",
    "    # token is 'O', prediction is something else: HYPOTHESIZE\n",
    "    elif token=='O' and token_prediction!='O':\n",
    "        metric_dict['total']['hypothesize'] += 1\n",
    "        metric_dict[token_prediction[2:]]['hypothesize'] += 1\n",
    "        \n",
    "    # token is not 'O', prediction is 'O': MISSED\n",
    "    elif token!='O' and token_prediction=='O':\n",
    "        metric_dict['total']['missed'] += 1\n",
    "        metric_dict[token[2:]]['missed'] += 1\n",
    "    \n",
    "    # token is not 'O', predictions is not 'O'\n",
    "    elif token!='O' and token_prediction!='O':\n",
    "        if token==token_prediction:\n",
    "            metric_dict['total']['correct'] += 1\n",
    "            metric_dict['total']['correct-not-O'] += 1\n",
    "            metric_dict[token[2:]]['correct'] += 1\n",
    "            metric_dict[token[2:]]['correct-not-O'] += 1\n",
    "        elif token[:2]==token_prediction[:2]:\n",
    "            metric_dict['total']['correct-type-only'] += 1\n",
    "            metric_dict[token[2:]]['correct-type-only'] += 1\n",
    "            \n",
    "    else:\n",
    "        print('An error has occured.')\n",
    "        print(token)\n",
    "        print(token_prediction)\n",
    "    \n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counter = {'O': 0, 'not-O': 0,\n",
    "                    'B-PER': 0, 'I-PER': 0, \n",
    "                    'B-LOC': 0, 'I-LOC': 0, \n",
    "                    'B-ORG': 0, 'I-ORG': 0}\n",
    "\n",
    "flair_counter = deepcopy(category_counter)\n",
    "bert_counter = deepcopy(category_counter)\n",
    "\n",
    "metrics = {\n",
    "    'correct': 0,          # exactly the same\n",
    "    'correct-not-O': 0,    # exactly the same and not 'O'\n",
    "    'incorrect': 0,        # not exactly the same\n",
    "    'missed': 0,           # token is not 'O', but 'O' is predicted\n",
    "    'hypothesize': 0,      # token is 'O', but something else is predicted\n",
    "    'correct-type-only': 0 # type not 'O' and is correct, disregarding IOB\n",
    "}\n",
    "\n",
    "flair_total_metrics = {\n",
    "    'total': deepcopy(metrics),\n",
    "    'PER': deepcopy(metrics),\n",
    "    'LOC': deepcopy(metrics),\n",
    "    'ORG': deepcopy(metrics)\n",
    "}\n",
    "\n",
    "bert_total_metrics = deepcopy(flair_total_metrics)\n",
    "\n",
    "for i, sentence in enumerate(categories):\n",
    "    for j, token in enumerate(sentence):\n",
    "        \n",
    "        category_counter[token] += 1\n",
    "        if token!='O':\n",
    "            category_counter['not-O'] += 1\n",
    "            \n",
    "        flair_counter[flair_preds[i][j]] += 1\n",
    "        if flair_preds[i][j]!='O':\n",
    "            flair_counter['not-O'] += 1\n",
    "            \n",
    "        bert_counter[bert_preds[i][j]] += 1\n",
    "        if bert_preds[i][j]!='O':\n",
    "            bert_counter['not-O'] += 1\n",
    "        \n",
    "        flair_total_metrics = compute_metrics(token, flair_preds[i][j], flair_total_metrics)\n",
    "        bert_total_metrics = compute_metrics(token, bert_preds[i][j], bert_total_metrics)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix, precision and recall computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_cm = np.array(confusion_matrix(categories_flattened, bert_preds_flattened, labels=labels))\n",
    "flair_cm = np.array(confusion_matrix(categories_flattened, flair_preds_flattened, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {\n",
    "    'O': 0,\n",
    "    'B-PER': 0, 'I-PER': 0, \n",
    "    'B-LOC': 0, 'I-LOC': 0, \n",
    "    'B-ORG': 0, 'I-ORG': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_acc = sum([bert_cm[i][i] for i in range(len(bert_cm))])/sum([item for sublist in bert_cm for item in sublist])\n",
    "flair_acc = sum([flair_cm[i][i] for i in range(len(flair_cm))])/sum([item for sublist in flair_cm for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_prec = deepcopy(types)\n",
    "flair_prec = deepcopy(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, key in enumerate(types.keys()):\n",
    "    bert_prec[key] = bert_cm[i][i]/sum(bert_cm.T[i])\n",
    "    flair_prec[key] = flair_cm[i][i]/sum(flair_cm.T[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_recall = deepcopy(types)\n",
    "flair_recall = deepcopy(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, key in enumerate(types.keys()):\n",
    "    bert_recall[key] = bert_cm[i][i]/sum(bert_cm[i])\n",
    "    flair_recall[key] = flair_cm[i][i]/sum(flair_cm[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 9462,\n",
       " 'not-O': 561,\n",
       " 'B-PER': 185,\n",
       " 'I-PER': 143,\n",
       " 'B-LOC': 95,\n",
       " 'I-LOC': 2,\n",
       " 'B-ORG': 90,\n",
       " 'I-ORG': 46}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
