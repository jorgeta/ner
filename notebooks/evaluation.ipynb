{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: BertNer and FlairNer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages, models, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# models\n",
    "from danlp.models import load_bert_ner_model, load_flair_ner_model\n",
    "\n",
    "# dataset\n",
    "from danlp.datasets import DDT\n",
    "\n",
    "# utils\n",
    "from flair.data import Sentence, Token\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-08 13:15:27,016 loading file /Users/jorgentaule/.danlp/flair.ner.pt\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "bert = load_bert_ner_model()\n",
    "flair = load_flair_ner_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data (splitted into a training set, a validation set, and a test set)\n",
    "ddt = DDT()\n",
    "train, valid, test = ddt.load_as_simple_ner(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the observations and the targets of the testset into new variables\n",
    "sentences, categories = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time models and get their test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_predictions():\n",
    "    start = time.time()\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        predictions.append(bert.predict(sentence)[1])\n",
    "    \n",
    "    time_spent = time.time()-start\n",
    "    \n",
    "    return predictions, time_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file <ipython-input-73-0ce36fe57670>\n",
      "NOTE: %mprun can only be used on functions defined in physical files, and not in the IPython environment.\n"
     ]
    }
   ],
   "source": [
    "bert_preds, bert_time_spent = get_bert_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 58.782347202301025, time per sentence: 0.1040395525704443\n"
     ]
    }
   ],
   "source": [
    "print(f'Time: {bert_time_spent}, time per sentence: {bert_time_spent/len(bert_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flair_predictions():\n",
    "    start = time.time()\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    flair_sentences = []\n",
    "    for sentence in sentences:\n",
    "        flair_sentence = Sentence()\n",
    "        for token in sentence:\n",
    "            flair_sentence.add_token(Token(token))\n",
    "        flair_sentences.append(flair_sentence)\n",
    "    flair.predict(flair_sentences)\n",
    "    \n",
    "    for s in flair_sentences:\n",
    "        predicted_categories = []\n",
    "        for t in s:\n",
    "            predicted_categories.append(t.tags['ner'].value)\n",
    "        predictions.append(predicted_categories)\n",
    "    \n",
    "    time_spent = time.time()-start\n",
    "    \n",
    "    return predictions, time_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgentaule/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/flair/embeddings.py:355: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  word_embedding, device=flair.device, dtype=torch.float\n"
     ]
    }
   ],
   "source": [
    "flair_preds, flair_time_spent = get_flair_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29.312140941619873, time per sentence: 0.05187989547189358\n"
     ]
    }
   ],
   "source": [
    "print(f'Time: {flair_time_spent}, time per sentence: {flair_time_spent/len(flair_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy, precision, recall og f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove MISC from the dataset, as this is not predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(categories):\n",
    "    for j, token in enumerate(sentence):\n",
    "        if token=='I-MISC' or token=='B-MISC':\n",
    "            categories[i][j] = 'O'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which situations are there?\n",
    "\n",
    "- Look at all the predictions similarly.\n",
    "- Only look at PER, ORG, LOC separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(token, token_prediction, metric_dict):\n",
    "    # count all incorrect predictions\n",
    "    if token!=token_prediction:\n",
    "        metric_dict['total']['incorrect'] += 1\n",
    "        if token!='O':\n",
    "            metric_dict[token[2:]]['incorrect'] += 1\n",
    "    \n",
    "    # both are 'O': add to correct\n",
    "    if token=='O' and token_prediction=='O':\n",
    "        metric_dict['total']['correct'] += 1\n",
    "    \n",
    "    # token is 'O', prediction is something else: HYPOTHESIZE\n",
    "    elif token=='O' and token_prediction!='O':\n",
    "        metric_dict['total']['hypothesize'] += 1\n",
    "        metric_dict[token_prediction[2:]]['hypothesize'] += 1\n",
    "        \n",
    "    # token is not 'O', prediction is 'O': MISSED\n",
    "    elif token!='O' and token_prediction=='O':\n",
    "        metric_dict['total']['missed'] += 1\n",
    "        metric_dict[token[2:]]['missed'] += 1\n",
    "    \n",
    "    # token is not 'O', predictions is not 'O'\n",
    "    elif token!='O' and token_prediction!='O':\n",
    "        if token==token_prediction:\n",
    "            metric_dict['total']['correct'] += 1\n",
    "            metric_dict['total']['correct-not-O'] += 1\n",
    "            metric_dict[token[2:]]['correct'] += 1\n",
    "            metric_dict[token[2:]]['correct-not-O'] += 1\n",
    "        elif token[:2]==token_prediction[:2]:\n",
    "            metric_dict['total']['correct-type-only'] += 1\n",
    "            metric_dict[token[2:]]['correct-type-only'] += 1\n",
    "            \n",
    "    else:\n",
    "        print('An error has occured.')\n",
    "        print(token)\n",
    "        print(token_prediction)\n",
    "    \n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counter = {'O': 0, 'not-O': 0,\n",
    "                    'B-PER': 0, 'I-PER': 0, \n",
    "                    'B-LOC': 0, 'I-LOC': 0, \n",
    "                    'B-ORG': 0, 'I-ORG': 0}\n",
    "\n",
    "flair_counter = deepcopy(category_counter)\n",
    "bert_counter = deepcopy(category_counter)\n",
    "\n",
    "metrics = {\n",
    "    'correct': 0,          # exactly the same\n",
    "    'correct-not-O': 0,    # exactly the same and not 'O'\n",
    "    'incorrect': 0,        # not exactly the same\n",
    "    'missed': 0,           # token is not 'O', but 'O' is predicted\n",
    "    'hypothesize': 0,      # token is 'O', but something else is predicted\n",
    "    'correct-type-only': 0 # type not 'O' and is correct, disregarding IOB\n",
    "}\n",
    "\n",
    "flair_total_metrics = {\n",
    "    'total': deepcopy(metrics),\n",
    "    'PER': deepcopy(metrics),\n",
    "    'LOC': deepcopy(metrics),\n",
    "    'ORG': deepcopy(metrics)\n",
    "}\n",
    "\n",
    "bert_total_metrics = deepcopy(flair_total_metrics)\n",
    "\n",
    "for i, sentence in enumerate(categories):\n",
    "    for j, token in enumerate(sentence):\n",
    "        \n",
    "        category_counter[token] += 1\n",
    "        if token!='O':\n",
    "            category_counter['not-O'] += 1\n",
    "            \n",
    "        flair_counter[flair_preds[i][j]] += 1\n",
    "        if flair_preds[i][j]!='O':\n",
    "            flair_counter['not-O'] += 1\n",
    "            \n",
    "        bert_counter[bert_preds[i][j]] += 1\n",
    "        if bert_preds[i][j]!='O':\n",
    "            bert_counter['not-O'] += 1\n",
    "        \n",
    "        flair_total_metrics = compute_metrics(token, flair_preds[i][j], flair_total_metrics)\n",
    "        bert_total_metrics = compute_metrics(token, bert_preds[i][j], bert_total_metrics)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': {'correct': 9886,\n",
       "  'correct-not-O': 515,\n",
       "  'incorrect': 137,\n",
       "  'missed': 91,\n",
       "  'hypothesize': 12,\n",
       "  'correct-type-only': 32},\n",
       " 'PER': {'correct': 308,\n",
       "  'correct-not-O': 308,\n",
       "  'incorrect': 10,\n",
       "  'missed': 8,\n",
       "  'hypothesize': 1,\n",
       "  'correct-type-only': 2},\n",
       " 'LOC': {'correct': 84,\n",
       "  'correct-not-O': 84,\n",
       "  'incorrect': 17,\n",
       "  'missed': 12,\n",
       "  'hypothesize': 4,\n",
       "  'correct-type-only': 4},\n",
       " 'ORG': {'correct': 123,\n",
       "  'correct-not-O': 123,\n",
       "  'incorrect': 98,\n",
       "  'missed': 71,\n",
       "  'hypothesize': 7,\n",
       "  'correct-type-only': 26}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_total_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': {'correct': 9896,\n",
       "  'correct-not-O': 535,\n",
       "  'incorrect': 127,\n",
       "  'missed': 53,\n",
       "  'hypothesize': 22,\n",
       "  'correct-type-only': 51},\n",
       " 'PER': {'correct': 306,\n",
       "  'correct-not-O': 306,\n",
       "  'incorrect': 12,\n",
       "  'missed': 10,\n",
       "  'hypothesize': 2,\n",
       "  'correct-type-only': 2},\n",
       " 'LOC': {'correct': 90,\n",
       "  'correct-not-O': 90,\n",
       "  'incorrect': 11,\n",
       "  'missed': 10,\n",
       "  'hypothesize': 4,\n",
       "  'correct-type-only': 1},\n",
       " 'ORG': {'correct': 139,\n",
       "  'correct-not-O': 139,\n",
       "  'incorrect': 82,\n",
       "  'missed': 33,\n",
       "  'hypothesize': 16,\n",
       "  'correct-type-only': 48}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_total_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 9383,\n",
       " 'not-O': 640,\n",
       " 'B-PER': 180,\n",
       " 'I-PER': 138,\n",
       " 'B-LOC': 96,\n",
       " 'I-LOC': 5,\n",
       " 'B-ORG': 161,\n",
       " 'I-ORG': 60}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 9414,\n",
       " 'not-O': 609,\n",
       " 'B-PER': 181,\n",
       " 'I-PER': 141,\n",
       " 'B-LOC': 108,\n",
       " 'I-LOC': 21,\n",
       " 'B-ORG': 123,\n",
       " 'I-ORG': 35}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 9462,\n",
       " 'not-O': 561,\n",
       " 'B-PER': 185,\n",
       " 'I-PER': 143,\n",
       " 'B-LOC': 95,\n",
       " 'I-LOC': 2,\n",
       " 'B-ORG': 90,\n",
       " 'I-ORG': 46}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some definitions\n",
    "\n",
    "- Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "- Precision = TP/(TP+FP)\n",
    "- Recall = TP/(TP+FN)\n",
    "- F1 Score = 2 * (Recall * Precision) / (Recall + Precision)\n",
    "\n",
    "(https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_preds_flattened = [item for sublist in flair_preds for item in sublist]\n",
    "bert_preds_flattened = [item for sublist in bert_preds for item in sublist]\n",
    "categories_flattened = [item for sublist in categories for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']\n",
    "\n",
    "bert_cm = np.array(confusion_matrix(categories_flattened, bert_preds_flattened, labels=labels))\n",
    "flair_cm = np.array(confusion_matrix(categories_flattened, flair_preds_flattened, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']\n",
      "[[9361    2    0    2    2   14    2]\n",
      " [  10  168    0    1    0    1    0]\n",
      " [   0    0  138    0    0    0    0]\n",
      " [   9    0    0   86    0    1    0]\n",
      " [   1    0    0    0    4    0    0]\n",
      " [  25   11    0   19    0  106    0]\n",
      " [   8    0    3    0   15    1   33]]\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(bert_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']\n",
      "[[9371    1    0    4    0    5    2]\n",
      " [   8  170    0    1    0    1    0]\n",
      " [   0    0  138    0    0    0    0]\n",
      " [  10    1    0   82    0    2    1]\n",
      " [   2    0    0    0    2    0    1]\n",
      " [  59   13    0    8    0   81    0]\n",
      " [  12    0    5    0    0    1   42]]\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(flair_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {\n",
    "    'O': 0,\n",
    "    'B-PER': 0, 'I-PER': 0, \n",
    "    'B-LOC': 0, 'I-LOC': 0, \n",
    "    'B-ORG': 0, 'I-ORG': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_acc = sum([bert_cm[i][i] for i in range(len(bert_cm))])/sum([item for sublist in bert_cm for item in sublist])\n",
    "flair_acc = sum([flair_cm[i][i] for i in range(len(flair_cm))])/sum([item for sublist in flair_cm for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9873291429711664, 0.9863314376933054)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_acc, flair_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_prec = deepcopy(types)\n",
    "flair_prec = deepcopy(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, key in enumerate(types.keys()):\n",
    "    bert_prec[key] = bert_cm[i][i]/sum(bert_cm.T[i])\n",
    "    flair_prec[key] = flair_cm[i][i]/sum(flair_cm.T[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0.9943700871043127,\n",
       " 'B-PER': 0.9281767955801105,\n",
       " 'I-PER': 0.9787234042553191,\n",
       " 'B-LOC': 0.7962962962962963,\n",
       " 'I-LOC': 0.19047619047619047,\n",
       " 'B-ORG': 0.8617886178861789,\n",
       " 'I-ORG': 0.9428571428571428}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0.9903825829634326,\n",
       " 'B-PER': 0.918918918918919,\n",
       " 'I-PER': 0.965034965034965,\n",
       " 'B-LOC': 0.8631578947368421,\n",
       " 'I-LOC': 1.0,\n",
       " 'B-ORG': 0.9,\n",
       " 'I-ORG': 0.9130434782608695}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_recall = deepcopy(types)\n",
    "flair_recall = deepcopy(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, key in enumerate(types.keys()):\n",
    "    bert_recall[key] = bert_cm[i][i]/sum(bert_cm[i])\n",
    "    flair_recall[key] = flair_cm[i][i]/sum(flair_cm[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0.9976553341148886,\n",
       " 'B-PER': 0.9333333333333333,\n",
       " 'I-PER': 1.0,\n",
       " 'B-LOC': 0.8958333333333334,\n",
       " 'I-LOC': 0.8,\n",
       " 'B-ORG': 0.6583850931677019,\n",
       " 'I-ORG': 0.55}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0.9987210913353938,\n",
       " 'B-PER': 0.9444444444444444,\n",
       " 'I-PER': 1.0,\n",
       " 'B-LOC': 0.8541666666666666,\n",
       " 'I-LOC': 0.4,\n",
       " 'B-ORG': 0.5031055900621118,\n",
       " 'I-ORG': 0.7}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_rep = classification_report(categories_flattened, bert_preds_flattened, labels=labels)\n",
    "flair_rep = classification_report(categories_flattened, flair_preds_flattened, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.99      1.00      1.00      9383\n",
      "       B-PER       0.93      0.93      0.93       180\n",
      "       I-PER       0.98      1.00      0.99       138\n",
      "       B-LOC       0.80      0.90      0.84        96\n",
      "       I-LOC       0.19      0.80      0.31         5\n",
      "       B-ORG       0.86      0.66      0.75       161\n",
      "       I-ORG       0.94      0.55      0.69        60\n",
      "\n",
      "    accuracy                           0.99     10023\n",
      "   macro avg       0.81      0.83      0.79     10023\n",
      "weighted avg       0.99      0.99      0.99     10023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bert_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.99      1.00      0.99      9383\n",
      "       B-PER       0.92      0.94      0.93       180\n",
      "       I-PER       0.97      1.00      0.98       138\n",
      "       B-LOC       0.86      0.85      0.86        96\n",
      "       I-LOC       1.00      0.40      0.57         5\n",
      "       B-ORG       0.90      0.50      0.65       161\n",
      "       I-ORG       0.91      0.70      0.79        60\n",
      "\n",
      "    accuracy                           0.99     10023\n",
      "   macro avg       0.94      0.77      0.83     10023\n",
      "weighted avg       0.99      0.99      0.98     10023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(flair_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting memory_profiler\n",
      "  Downloading memory_profiler-0.58.0.tar.gz (36 kB)\n",
      "Requirement already satisfied: psutil in /Users/jorgentaule/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages (from memory_profiler) (5.4.7)\n",
      "Building wheels for collected packages: memory-profiler\n",
      "  Building wheel for memory-profiler (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for memory-profiler: filename=memory_profiler-0.58.0-py3-none-any.whl size=29147 sha256=a3119a589005368a0c7d809c2494be950d1e9a8d9449431dd7c5ddd55a88f5b8\n",
      "  Stored in directory: /Users/jorgentaule/Library/Caches/pip/wheels/56/19/d5/8cad06661aec65a04a0d6785b1a5ad035cb645b1772a4a0882\n",
      "Successfully built memory-profiler\n",
      "Installing collected packages: memory-profiler\n",
      "Successfully installed memory-profiler-0.58.0\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/Users/jorgentaule/.pyenv/versions/anaconda3-5.3.1/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at sentences where the models do wrong predictions (new notebook?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
